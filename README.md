# PySpark-ETL-Project-Build-a-Data-Pipeline-using-S3-and-MySQL
This project mainly focuses on the integration of PySpark with Amazon S3 and MySQL database to perform ETL(Extract-Transform-Load) and ELT(Extract-Load-Transform) operations.   

##  Why we need Integration with Spark!

   - To make faster data Processing.
   - Some technology does not work like spark technology.
   - Spark is a popular technology for big data processing.
   - Sometimes we need to process petabytes of data but data is stored in a particular server. When we process data on that          server here it will take lot of time. In that case we need to integration with spark to fast data processing.
   - Hadoop process data in 100 min but when we integrate spark with Hadoop then it will take to process data in 10 min.
   
 ##  PySpark Integration with S3!
 
   - S3 stored different kinds of data in buckets from various data sources thatâ€™s why data is in huge amounts.
   -  Easy to integrate with spark.
   -  We need to Integrate Spark with S3 to fast data processing.
   -  Using spark easy to read and write data from s3.
   -  In spark we need to create DataFrame from external data source S3.




